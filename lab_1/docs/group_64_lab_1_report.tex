%% Determines the type of document (including standard settings for layout).
\documentclass[letterpaper]{article}



%% Package to control the size of a page: the standard margins used by LaTeX are
%% very wide!
\usepackage[margin=1in]{geometry}


%% Packages add functionality to the document. The AMS packages are standard
%% packages to support various mathematical notations. AMS stands for American
%% Mathematical Society, the organization that maintains these packages.
\usepackage{amsmath,amsthm,amssymb}

%% Theorems-like environments using functionality provided by amsthm.
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
    %% [section] at the end specifies that theorems should be numbered
    %% per-section: Section x starts with theorem-like x.1 and so on, ...
\newtheorem{proposition}[theorem]{Proposition}
    %% [theorem] in the middle specifies: use the same counter as the theorem
    %% environment: here we number all theorem-like environments consecutively.
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\theoremstyle{remark}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}


%% Support for nicely formatted tables.
\usepackage{booktabs}


%% Support for colors & colors in tables.
\usepackage[table]{xcolor}

% Seven colors safe for use color blindness.
% Colors taken from doi:10.1038/nmeth.1618.
\definecolor{cbOrange}{RGB}{230,159,0}
\definecolor{cbSkyBlue}{RGB}{86,180,233}
\definecolor{cbBluishGreen}{RGB}{0,158,115}
\definecolor{cbYellow}{RGB}{240,228,66}
\definecolor{cbBlue}{RGB}{0,114,178}
\definecolor{cbVermillion}{RGB}{213,94,0}
\definecolor{cbReddischPurple}{RGB}{204,121,167}


%% Notation used in this document.
\newcommand{\n}{\mathbf{n}} %% Num. Replicas.
\newcommand{\f}{\mathbf{f}} %% Num. Faulty Replicas.

%% Misc. Math notation.
\newcommand{\BigO}{\mathcal{O}}
\newcommand{\abs}[1]{\lvert #1 \rvert}
\newcommand{\AName}[1]{\textsc{#1}}
\newcommand{\Var}[1]{\texttt{#1}}


%% Algorithms.
\usepackage{algorithm}
\usepackage[noend]{algorithmic}
\newcommand{\GETS}{:=}


%% Formatting SI-units.
\usepackage{siunitx}
\sisetup{per-mode=symbol}


%% TikZ: for creating figures.
\usepackage{tikz}

%% Configuration for figures: Nicer arrows.
\usetikzlibrary{arrows.meta}
\tikzset{>=Stealth}


%% pgfplots: drawing plots using TikZ.
\usepackage{pgfplots}
%% Configuration for plots: Use color-blind friendly colors.
\pgfplotscreateplotcyclelist{cbSafeList}{
    very thick,solid,cbOrange,every mark/.append style={solid},mark=*\\
    very thick,solid,cbSkyBlue,every mark/.append style={solid},mark=*\\
    very thick,solid,cbBluishGreen,every mark/.append style={solid},mark=*\\
    very thick,solid,cbYellow,every mark/.append style={solid},mark=*\\
    very thick,solid,cbBlue,every mark/.append style={solid},mark=*\\
    very thick,solid,cbVermillion,every mark/.append style={solid},mark=*\\
    very thick,solid,cbReddischPurple,every mark/.append style={solid},mark=*\\
    very thick,solid,black,every mark/.append style={solid},mark=*\\
}
\pgfplotsset{
    legend style={font=\small},
    compat=1.16,
    width=260pt,
    height=140pt,
    legend cell align=left,
    xlabel near ticks,
    ylabel near ticks,
    every axis/.append style={
        cycle list name=cbSafeList,
        ymin=0,
        enlargelimits=0.05,
        mark size=1pt,
        ylabel style={align=center},
        xlabel style={align=center},
        title style={align=center}
    }
}

%% PgfplotsTable: loading data files to use with pgfplots.
\usepackage{pgfplotstable}


%% Support for hyperlinks and urls. The setting ``colorlinks'' sets how links
%% are shown in the document (with a color, without underline). We put hyperref
%% last---it has a tendency to break other packages when loaded before them.
\usepackage[colorlinks]{hyperref}
\hypersetup{
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue
}
\usepackage{tocloft}
% Make Table of Contents and List of Figures text blue
\renewcommand{\cftsecfont}{\color{blue}}
\renewcommand{\cftsecpagefont}{\color{blue}}

\renewcommand{\cftfigfont}{\color{blue}}
\renewcommand{\cftfigpagefont}{\color{blue}}
\usepackage{graphicx}
\usepackage{pgffor}
\usepackage{caption}
\usepackage{tabularx}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{float}

\begin{document}

\begin{titlepage}

    \thispagestyle{empty}
    \centering
    \vspace*{3em}

    \Huge{COMPSCI 2XC3 Lab Report 1}\\[1em]
    \Large Prepared by\\[2em]
    \Large Group 64\\[2em] 

    \LARGE Luca Mawyin\\[0.5em]
    \LARGE Anderson Ray\\[0.5em]
    \LARGE Theo Pham\\[2em]
    
    \Large COMPSCI 2ME3 \\[0.5em]
    \Large McMaster University\\[2em]     
    \Large \today\\[2cm]

\end{titlepage}

\tableofcontents
\newpage

\listoffigures
\newpage

\newpage
\section*{Executive Summary}

\begin{itemize}[leftmargin=*]
    \item Basic experiments show that sorting algorithms with quadratic time complexity become slow very quickly on random data, while algorithms with logarithmic time complexity scale much better as input size increases.
    (\hyperref[exp1]{Experiment 1}, \hyperref[exp4]{Experiment 4})

    \item The performance of quadratic time algorithms depends strongly on the input structure. Insertion Sort performs very well on small or nearly sorted lists, despite its worse theoretical complexity.
    (\hyperref[exp3]{Experiment 3}, \hyperref[exp8]{Experiment 8})

    \item Simple optimizations that reduce constant overhead can noticeably improve real world performance, even when the overall time complexity does not change.
    (\hyperref[exp2]{Experiment 2})

    \item Quick Sort is sensitive to input order because of its pivot selection, while Merge Sort behaves more consistently across different types of inputs.
    (\hyperref[exp5]{Experiment 5}, \hyperref[exp8]{Experiment 8})

    \item Using an iterative bottom up approach slightly improves Merge Sort performance by avoiding recursion overhead, although the improvement is limited.
    (\hyperref[exp7]{Experiment 7})

    \item More complex optimizations, such as dual pivot Quick Sort, do not provide enough performance improvement to justify their added implementation complexity.
    (\hyperref[exp6]{Experiment 6})

    \item Overall, the experiments show that practical sorting implementations benefit from hybrid strategies that choose different algorithms based on input size and structure.
    (\hyperref[exp2]{Experiment 7}, \hyperref[exp8]{Experiment 8})
\end{itemize}

\newpage
% Experiment 1 
\stepcounter{section}
\section*{Experiment \thesection}
\label{exp1}
\addcontentsline{toc}{section}{Experiment 1: Runtime comparison of Bubble, Selection, and Insertion Sort}

In our experiment comparing Bubble Sort, Insertion Sort, and Selection Sort. We ran 100 tests for each sorting algorithm going from a list length of 0, to a list length of 1000 each list length being 10 elements longer then the last.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Figure_\thesection.png}
    \caption{List Length vs. Time (ms) for Bubble, Selection, and Insertion Sort}
    \label{fig:sort_graph_\thesection}
\end{figure}

Looking at the slopes, each algorithm looks to have a parabolic shape which makes sense as we know that the algorithms used are $\BigO(n^2)$.

Bubble sort is the slowest as the inner loop, loops through the entire list every iteration of the outer loop. Selection sort is faster then Insertion sort as swaps elements in the list smarter using less memory reads and writes making it faster.

% Experiment 2
\stepcounter{section}
\section*{Experiment \thesection}
\label{exp2}
\addcontentsline{toc}{section}{Experiment 2: Comparison of original and optimized Bubble and Selection Sort implementations}


In this experiment, we compared the original Bubble Sort and Selection Sort with their optimized variations.
The Bubble Sort variation reduces the number of swaps by shifting elements, while the Selection Sort variation places both the minimum and maximum elements in each iteration.
\par
The experiment was performed on randomly generated lists with lengths ranging from 0 to 1000, increasing by 10 each time.
All lists contained integers between 0 and 100000, and runtimes were measured in milliseconds.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Figure_2a_bubble.png}
    \caption{List Length vs. Time (ms) for original Bubble Sort and its optimized variation}
    \label{fig:bubble_exp2}
\end{figure}

Figure~\ref{fig:bubble_exp2} shows that the optimized Bubble Sort consistently outperforms the original version as the list size increases.
This improvement is due to the reduced number of swap operations, which lowers the cost of memory accesses.
Although both implementations exhibit quadratic growth, the optimization significantly improves performance in practice.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Figure_2b_Selection.png}
    \caption{List Length vs. Time (ms) for original Selection Sort and the min+max variation}
    \label{fig:selection_exp2}
\end{figure}

As shown in Figure~\ref{fig:selection_exp2}, the min max variation of Selection Sort provides a modest but consistent speedup over the original implementation.
By placing two elements per iteration, the number of outer loop passes is reduced.
However, the overall time complexity remains quadratic, limiting the performance improvement.
\par
In summary, both optimizations reduce constant overhead rather than asymptotic complexity.
The Bubble Sort variation yields a more noticeable improvement, while the Selection Sort variation offers a smaller but stable performance gain.

% Experiment 3
\stepcounter{section}
\section*{Experiment \thesection}
\label{exp3}
\addcontentsline{toc}{section}{Experiment 3: Performance comparison on near sorted lists with varying numbers of swaps}

In our experiment comparing Bubble Sort, Insertion Sort, and Selection Sort on sorted lists with varying numbers of swaps made. We ran 109 tests for each sorting algorithm on lists of size 2000 with the number of swaps ranging from 0 - 10965. Each test would increase the number of swaps made by 100.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Figure_3.png}
    \caption{List Length vs. Number of Swaps on a Sorted List for Bubble, Selection, and Insertion Sort.}
    \label{fig:sort_graph_swaps}
\end{figure}

Looking at the graph, Bubble Sort and Insertion Sort preform better with less swaps. Bubble Sort preforms better with less swaps, as its doing much less memory reads and writes in the innerloop. Insertion swap preforms better with less swaps as it exits out the second loop if it runs into sorted pairs of elements which are more common with less swaps.

Selection Sort doesn't perform better with less swaps as the number of checks and swaps are independent of whether the list is sorted or not. Even if the innerloop can't find a min index past L[i] it still swaps L[i] with itself.

% Experiment 4
\stepcounter{section}
\section*{Experiment \thesection}
\label{exp4}
\addcontentsline{toc}{section}{Experiment 4: Runtime comparison of Merge Sort, Quick Sort, and Heap Sort}


For this experiment, we compared the runtime of Quick Sort, Merge Sort, and Heap Sort. There were 100 tests run for each algorithm, with increasing list sizes from 0 to 1000, incrementing the list length by ten each iteration.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Figure_\thesection.png}
    \caption{List Length vs. Time (ms) for Quick, Merge, and Heap Sort}
    \label{fig:1sort_graph_\thesection}
\end{figure}

Observing the resulting runtimes of each algorithm, they appear to be near linear with a slight curve. This is an accurate depiction as each of the sorting algorithms represented have a runtime of $\BigO(n\log{n})$

Heap Sort is the slowest sorting algorithm due to the constant overhead required in heap maintenance. Merge Sort is slower than Quick Sort due to the increased overhead of copying elements, along with having to zip sorted partitions of lists together, as opposed to centering the sort around a pivot and copying elements less. 

% Experiment 5
\stepcounter{section}
\section*{Experiment \thesection}
\label{exp5}
\addcontentsline{toc}{section}{Experiment 5: Performance comparison on near sorted lists for good sorting algorithms}


In this experiment, Quick Sort, Merge Sort, and Heap Sort were compared using near sorted lists of length 1000. There were 100 tests run for each algorithm, with an increasing number of swaps on an already-sorted list. The number of swaps ranged from 0 to 500, with an incrementation of five. Furthermore, each increment was executed five times, with the runtime averaged out in order to guarantee a higher accuracy.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Figure_\thesection.png}
    \caption{Swaps vs. Time (ms) for Quick, Merge, and Heap Sort}
    \label{fig:2sort_graph_\thesection}
\end{figure}

Observing the runtime of each algorithm on sorted lists relative to the number of swaps performed, Quick Sort began to outperform the other sorting algorithms when the number of swaps reached 100. As a more general observation, the performance of Quick Sort was more appealing when the number of swaps was 10\% of the length of the list. This is likely due to the fact that having a swap rate of 10\% introduces enough randomness into the list to nearly guarantee that the first element in the list is substantially closer to the median value, or at the very least further from the least value of the list.

% Experiment 6
\stepcounter{section}
\section*{Experiment \thesection}
\label{exp6}
\addcontentsline{toc}{section}{Experiment 6: Comparison of single pivot and dual pivot Quick Sort}


This experiment compares a typical Quick Sort to a Quick Sort variant that has two pivot points. The experiment was performed on lists of increasing sizes from 0 to 1000, incrementing the length by ten each iteration. Each iteration was performed five times, with the result being the average of the runtimes for improved accuracy.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Figure_\thesection.png}
    \caption{Swaps vs. Time (ms) for Single and Dual Quick Sort}
    \label{fig:3sort_graph_\thesection}
\end{figure}

Observing the runtime, the difference between Quick Sort with a single pivot versus with a dual pivot does not seem substantial enough to warrant implementation. However, as the list lengths used in the experiment reached 1000 elements, even though the absolute values of runtimes between Quick Sort and Dual Quick Sort were approximately 0.1ms, this denotes a 20\% improvement in performance. Given the substantial improvement in relative performance, this implementation may prove useful with data sets of lengths that substantially exceed those used in the experiment.

% Experiment 7
\stepcounter{section}
\section*{Experiment \thesection}
\label{exp7}
\addcontentsline{toc}{section}{Experiment 7: Comparison of recursive and bottom up Merge Sort implementations}


In this experiment, we compared the traditional recursive implementation of Merge Sort with a bottom up iterative version.
The bottom up approach eliminates recursion by iteratively merging sublists using an increasing window size.
\par
The experiment was conducted on randomly generated lists with lengths ranging from 0 to 5000, increasing by 50 each iteration.
Each configuration was executed five times, and the minimum runtime was recorded to reduce noise from system interference.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Figure_7.png}
    \caption{List Length vs. Time (ms) for recursive and bottom up Merge Sort}
    \label{fig:merge_exp7}
\end{figure}

As shown in Figure~\ref{fig:merge_exp7}, the bottom up Merge Sort consistently outperforms the recursive version across all tested list sizes.
The improvement is modest, as both implementations share the same $\BigO(n\log n)$ time complexity.
Most of the runtime cost comes from the linear merging process rather than recursion overhead, which limits the overall performance gain of the iterative approach.


% Experiment 8
\stepcounter{section}
\section*{Experiment \thesection}
\label{exp8}
\addcontentsline{toc}{section}{Experiment 8: Comparison of Insertion Sort with Merge and Quick Sort on small near sorted lists}


In this experiment, we compared Insertion Sort with Merge Sort and Quick Sort on small, near sorted lists to determine when
a quadratic time algorithm can outperform more efficient sorting algorithms. The experiment was conducted on lists with lengths ranging from 5 to 300,
increasing by 5 each iteration.
\par
To favor Insertion Sort, the lists were generated to be near sorted using a small number of random swaps.
Each configuration was executed five times, and the minimum runtime was recorded.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Figure_8.png}
    \caption{List Length vs. Time (ms) for Insertion, Merge, and Quick Sort on small near sorted lists}
    \label{fig:exp8}
\end{figure}

As shown in Figure~\ref{fig:exp8}, Insertion Sort outperforms Merge Sort and Quick Sort for very small list sizes due to its low constant overhead and
efficiency on near sorted data. However, as the list size increases, the quadratic time complexity of Insertion Sort dominates, causing it to become
slower than the other algorithms. These results demonstrate why practical sorting implementations often use hybrid approaches,
applying Insertion Sort on small sublists and switching to Merge or Quick Sort for larger inputs.

\newpage
\section*{Appendix: Code Organization}

All source code for this lab is located under the \texttt{lab\_1/src/} directory.

\begin{itemize}[leftmargin=*]
    \item \texttt{sorting/bad\_sorts.py}: Contains implementations of Bubble Sort, Selection Sort, Insertion Sort, and their optimized variants.
    \item \texttt{sorting/good\_sorts.py}: Contains implementations of Merge Sort, Quick Sort, Heap Sort, DualvPivot Quick Sort, and Bottom Up Merge Sort.
    \item \texttt{experiments/experiment\_1.py}: Runtime comparison of Bubble, Selection, and Insertion Sort.
    \item \texttt{experiments/experiment\_2.py}: Comparison of original and optimized Bubble and Selection Sort implementations.
    \item \texttt{experiments/experiment\_3.py}: Performance comparison on near sorted lists with varying numbers of swaps.
    \item \texttt{experiments/experiment\_4.py}: Runtime comparison of Merge Sort, Quick Sort, and Heap Sort.
    \item \texttt{experiments/experiment\_5.py}: Performance comparison on near sorted lists for good sorting algorithms.
    \item \texttt{experiments/experiment\_6.py}: Comparison of single pivot and dual pivot Quick Sort.
    \item \texttt{experiments/experiment\_7.py}: Comparison of recursive and bottom up Merge Sort implementations.
    \item \texttt{experiments/experiment\_8.py}: Comparison of Insertion Sort with Merge and Quick Sort on small near sorted lists.
\end{itemize}


\end{document}